{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5237eaa",
   "metadata": {},
   "source": [
    "<b> Fine-tune a pretrained BERT model on the polyglot-ner data to perform named entity recognition. The data consist multiple language. One language will be selected to fine-tune the model. The fine-tuning will performed 3 times. 1- with 1000 sentences, 2- with 300 sentences. 3- with 3000 sentences and frozen embedding</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9211fbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'lang', 'words', 'ner'],\n",
      "        num_rows: 21070925\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from datasets import load_dataset\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Load the Polyglot-NER dataset from Hugging Face's datasets library\n",
    "polyglot_ner_dataset = load_dataset(\"polyglot_ner\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(polyglot_ner_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff980d0",
   "metadata": {},
   "source": [
    "<b> We want to fine-tune the BERT model on one of the languages of the dataset that:  1- is not English, 2- Has already a pretrained BERT-base. 3- The language contains at least 7k sentences. The following code block will attempt to find a language with these conditions. </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2583bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: et, Sentence Count: 87023\n",
      "Language: nl, Sentence Count: 520664\n",
      "Language: es, Sentence Count: 386699\n",
      "Language: ko, Sentence Count: 560105\n",
      "Language: el, Sentence Count: 446052\n",
      "Language: hr, Sentence Count: 629667\n",
      "Language: id, Sentence Count: 463862\n",
      "Language: uk, Sentence Count: 561373\n",
      "Language: hu, Sentence Count: 590218\n",
      "Language: ca, Sentence Count: 372665\n",
      "Language: fr, Sentence Count: 418411\n",
      "Language: tl, Sentence Count: 160750\n",
      "Language: th, Sentence Count: 217631\n",
      "Language: bg, Sentence Count: 559694\n",
      "Language: pt, Sentence Count: 396773\n",
      "Language: sk, Sentence Count: 500135\n",
      "Language: vi, Sentence Count: 351643\n",
      "Language: ru, Sentence Count: 551770\n",
      "Language: de, Sentence Count: 547578\n",
      "Language: fi, Sentence Count: 387465\n",
      "Language: cs, Sentence Count: 564462\n",
      "Language: he, Sentence Count: 459933\n",
      "Language: da, Sentence Count: 546440\n",
      "Language: sv, Sentence Count: 634881\n",
      "Language: fa, Sentence Count: 492903\n",
      "Language: ar, Sentence Count: 339109\n",
      "Language: lv, Sentence Count: 331568\n",
      "Language: zh, Sentence Count: 1570853\n",
      "Language: lt, Sentence Count: 848018\n",
      "Language: ro, Sentence Count: 285985\n",
      "Language: sl, Sentence Count: 521251\n",
      "Language: sr, Sentence Count: 559423\n",
      "Language: en, Sentence Count: 423982\n",
      "Language: ms, Sentence Count: 528181\n",
      "Language: ja, Sentence Count: 1691018\n",
      "Language: pl, Sentence Count: 623267\n",
      "Language: it, Sentence Count: 378325\n",
      "Language: hi, Sentence Count: 401648\n",
      "Language: no, Sentence Count: 552176\n",
      "Language: tr, Sentence Count: 607324\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the number of sentences for each language\n",
    "language_counts = Counter(polyglot_ner_dataset['train']['lang'])\n",
    "\n",
    "# Display the counts for each language\n",
    "for language, count in language_counts.items():\n",
    "    print(f\"Language: {language}, Sentence Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102e931",
   "metadata": {},
   "source": [
    "<b> \"ar\" Arabic language will be used to fine-tune the pretrained Bert-model https://huggingface.co/aubmindlab/bert-base-arabertv02. Next the data will be prepared</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b85e4495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████| 21070925/21070925 [09:50<00:00, 35681.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'lang', 'words', 'ner'],\n",
      "    num_rows: 339109\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to include only Arabic (ar) entries\n",
    "arabic_ner_dataset = polyglot_ner_dataset['train'].filter(lambda example: example['lang'] == 'ar')\n",
    "\n",
    "# Print basic information about the Arabic subset\n",
    "print(arabic_ner_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85dc94",
   "metadata": {},
   "source": [
    "<b> Now the entire data variable will be removed to fee some memory</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02873c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "\n",
    "del polyglot_ner_dataset\n",
    "del language_counts\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6c3c7",
   "metadata": {},
   "source": [
    "<b> Next is preparing the Arabic data. We will start by understanding the data and then tokenization using the same tokenizer of the model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36070d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['11408797', '11408798'], 'lang': ['ar', 'ar'], 'words': [['جيمس', 'ويليام', 'دنفر', '(', 'James', 'William', 'Denver', ')', '(', 'ولد', 'في', '23', 'أكتوبر', '،', '1817', 'وتوفي', 'في', '9', 'أغسطس', '1892', ')،', 'كان', 'سياسي', 'ا', 'أمريكيا', 'و', 'جندي', 'ا', 'و', 'محاميا', 'و', 'ممثلا', 'قديرا', '.'], ['أندري', 'أيوو', '،', 'من', 'مواليد', '17', 'ديسمبر', '1989', 'في', 'سيكلين', 'في', 'فرنسا', '،', 'لاعب', 'كرة', 'قدم', 'غاني', '.']], 'ner': [['PER', 'PER', 'PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O']]}\n"
     ]
    }
   ],
   "source": [
    "print(arabic_ner_dataset[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59848f80",
   "metadata": {},
   "source": [
    "<b> Next, will encode the data using the tokenizer that correspond with the BERT model. We'll also align the NER tags with the tokenized input. This is important because BERT's tokenizer might split a single word into multiple subwords</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb02e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 339109/339109 [00:53<00:00, 6320.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
    "\n",
    "# Define a function to tokenize and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['words'], truncation=True, padding='max_length', is_split_into_words=True, max_length=128)\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Use label dictionary to convert string labels to int\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            else:\n",
    "                # For subwords/wordpieces, set label to -100 (ignored in loss)\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Create a label_to_id dictionary\n",
    "label_to_id = {label: i for i, label in enumerate(set([lbl for sublist in arabic_ner_dataset['ner'] for lbl in sublist]))}\n",
    "\n",
    "# Apply the function to tokenize and align labels\n",
    "tokenized_arabic_ner_dataset = arabic_ner_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n",
    "# Now `tokenized_arabic_ner_dataset` is ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed64eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '11408797', 'lang': 'ar', 'words': ['جيمس', 'ويليام', 'دنفر', '(', 'James', 'William', 'Denver', ')', '(', 'ولد', 'في', '23', 'أكتوبر', '،', '1817', 'وتوفي', 'في', '9', 'أغسطس', '1892', ')،', 'كان', 'سياسي', 'ا', 'أمريكيا', 'و', 'جندي', 'ا', 'و', 'محاميا', 'و', 'ممثلا', 'قديرا', '.'], 'ner': ['PER', 'PER', 'PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'input_ids': [2, 11846, 19502, 39789, 14, 47, 14609, 6055, 60, 31733, 19674, 250, 32590, 179, 37170, 15, 14, 4254, 305, 2474, 3326, 103, 43611, 187, 21480, 305, 30, 4914, 13873, 234, 15, 103, 418, 4191, 112, 31399, 139, 8082, 112, 139, 36205, 139, 8365, 48794, 181, 20, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 3, 3, 3, 2, 2, -100, -100, 2, -100, -100, -100, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, -100, 2, -100, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_arabic_ner_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10756666",
   "metadata": {},
   "source": [
    "<b> Now the data is tokenized, we will load the pretrained model, define a trainer function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40666ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
    "\n",
    "# Load the pretrained model with a token classification head\n",
    "num_labels = len(label_to_id)\n",
    "model = BertForTokenClassification.from_pretrained(\"aubmindlab/bert-base-arabertv02\", num_labels=num_labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639a720",
   "metadata": {},
   "source": [
    "<b> First we will fie-tune the model using only 1000 sentences of the data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04707d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 01:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.094223</td>\n",
       "      <td>0.966894</td>\n",
       "      <td>0.966894</td>\n",
       "      <td>0.677510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.089688</td>\n",
       "      <td>0.971859</td>\n",
       "      <td>0.971859</td>\n",
       "      <td>0.661789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.092199</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.689740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09219853579998016, 'eval_accuracy': 0.9720434062902336, 'eval_f1_micro': 0.9720434062902336, 'eval_f1_macro': 0.6897397529127746, 'eval_runtime': 1.5635, 'eval_samples_per_second': 127.914, 'eval_steps_per_second': 2.558, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create a subset of the dataset for training (first 1000 sentences)\n",
    "train_subset_1k = tokenized_arabic_ner_dataset.select(range(1000))\n",
    "\n",
    "# Create a subset for testing (next 200 sentences)\n",
    "test_subset = tokenized_arabic_ner_dataset.select(range(1000, 1200))\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Flatten the lists and exclude labels for special tokens (i.e., -100)\n",
    "    flat_labels = [label for sublist in labels for label in sublist if label != -100]\n",
    "    flat_preds = [pred for sublist, label_sublist in zip(preds, labels) for pred, label in zip(sublist, label_sublist) if label != -100]\n",
    "\n",
    "    accuracy = accuracy_score(flat_labels, flat_preds)\n",
    "    f1_micro = f1_score(flat_labels, flat_preds, average='micro')\n",
    "    f1_macro = f1_score(flat_labels, flat_preds, average='macro')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize the Trainer with the training subset, test subset, and compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset_1k,\n",
    "    eval_dataset=test_subset,  \n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./my_fine_tuned_arabert_1k\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf71949",
   "metadata": {},
   "source": [
    "<b> Next, we will fine-tune the model but with 3000 examples instead of 1000 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c347eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 04:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.082661</td>\n",
       "      <td>0.968054</td>\n",
       "      <td>0.968054</td>\n",
       "      <td>0.770795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.082629</td>\n",
       "      <td>0.971450</td>\n",
       "      <td>0.971450</td>\n",
       "      <td>0.793495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.085617</td>\n",
       "      <td>0.972205</td>\n",
       "      <td>0.972205</td>\n",
       "      <td>0.796702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08561719954013824, 'eval_accuracy': 0.9722047541189788, 'eval_f1_micro': 0.9722047541189788, 'eval_f1_macro': 0.7967018511261825, 'eval_runtime': 2.3795, 'eval_samples_per_second': 126.075, 'eval_steps_per_second': 2.101, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Create a subset of the dataset for training (first 1000 sentences)\n",
    "train_subset_3k = tokenized_arabic_ner_dataset.select(range(3000))\n",
    "\n",
    "# Create a subset for testing (next 200 sentences)\n",
    "test_subset_3k = tokenized_arabic_ner_dataset.select(range(3000, 3300))\n",
    "\n",
    "# Initialize the Trainer with the training subset, test subset, and compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset_3k,\n",
    "    eval_dataset=test_subset_3k,  \n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./my_fine_tuned_arabert_3k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b67b0",
   "metadata": {},
   "source": [
    "<b> Lastely, we will fine-tune the model with 3000 sentences again, but with frozen embedding. This means that the embedding weights of the pretrained model are kept as they are \"frozen\". Only the other weights of the model will be changed. This approach is useful when having small dataset and to avoid overfitting</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c3d9174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 03:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.078664</td>\n",
       "      <td>0.968983</td>\n",
       "      <td>0.968983</td>\n",
       "      <td>0.676443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.071621</td>\n",
       "      <td>0.970937</td>\n",
       "      <td>0.970937</td>\n",
       "      <td>0.730858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.071630</td>\n",
       "      <td>0.972890</td>\n",
       "      <td>0.972890</td>\n",
       "      <td>0.757397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0716298520565033, 'eval_accuracy': 0.9728904628159727, 'eval_f1_micro': 0.9728904628159727, 'eval_f1_macro': 0.7573972146390583, 'eval_runtime': 2.3612, 'eval_samples_per_second': 127.053, 'eval_steps_per_second': 2.118, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
    "\n",
    "# Load the pretrained model \n",
    "num_labels = len(label_to_id)\n",
    "model = BertForTokenClassification.from_pretrained(\"aubmindlab/bert-base-arabertv02\", num_labels=num_labels)\n",
    "\n",
    "# Freeze the embeddings. This is where we found how to do it: https://discuss.huggingface.co/t/how-to-freeze-some-layers-of-bertmodel/917\n",
    "for param in model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define training arguments (can be the same or adjusted)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Create a  subset of the dataset for training \n",
    "train_subset_3k = tokenized_arabic_ner_dataset.select(range(3000, 6000))\n",
    "\n",
    "# Create a subset for testing \n",
    "test_subset = tokenized_arabic_ner_dataset.select(range(6000, 6300))\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset_3k,\n",
    "    eval_dataset=test_subset,\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./my_fine_tuned_arabert_3k_frozen_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841f969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
